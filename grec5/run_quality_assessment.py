#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆè´¨é‡è¯„ä¼°å·¥å…·
æ•´åˆä»£ç è´¨é‡æ£€æŸ¥ã€è‡ªåŠ¨åŒ–æµ‹è¯•ã€å®‰å…¨å®¡è®¡
ç¬¦åˆè¯„ä¼°æ ‡å‡†ä¸­çš„ä»£ç è´¨é‡å’Œä»£ç è°ƒè¯•è¦æ±‚
"""

import os
import sys
import json
import time
from datetime import datetime
from .code_standards import CodeStandardsChecker


def run_comprehensive_assessment():
    """è¿è¡Œç»¼åˆè´¨é‡è¯„ä¼°"""
    print("ğŸ† 5Gæ¨èå·¥å…· - ç»¼åˆè´¨é‡è¯„ä¼°")
    print("=" * 60)
    print(f"ğŸ“… è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # é¡¹ç›®æ–‡ä»¶åˆ—è¡¨
    project_files = [
        'recommendation_engine.py',
        'data_sources.py',
        'api_server.py',
        'gui.py',
        'config.json',
        'requirements.txt'
    ]
    
    assessment_results = {
        'timestamp': datetime.now().isoformat(),
        'code_quality': {},
        'code_debugging': {},
        'overall_score': 0
    }
    
    # 1. ä»£ç è´¨é‡è¯„ä¼° (10åˆ†)
    print("\nğŸ“ 1. ä»£ç è´¨é‡è¯„ä¼°")
    print("-" * 40)
    
    try:
        checker = CodeStandardsChecker()
        quality_result = checker.check_project(project_files)
        
        print(f"âœ… æ£€æŸ¥å®Œæˆ: {quality_result['files_checked']} ä¸ªæ–‡ä»¶")
        print(f"ğŸ“Š ä»£ç ç»Ÿè®¡:")
        print(f"   ğŸ“ æ€»è¡Œæ•°: {quality_result['total_metrics']['lines']}")
        print(f"   ğŸ”§ å‡½æ•°æ•°: {quality_result['total_metrics']['functions']}")
        print(f"   ğŸ—ï¸  ç±»æ•°: {quality_result['total_metrics']['classes']}")
        print(f"   ğŸ“– æ–‡æ¡£è¦†ç›–ç‡: {quality_result['total_metrics']['docstring_coverage']}%")
        
        print(f"ğŸš¨ é—®é¢˜ç»Ÿè®¡:")
        print(f"   âŒ é”™è¯¯: {quality_result['total_issues']['error']}")
        print(f"   âš ï¸  è­¦å‘Š: {quality_result['total_issues']['warning']}")
        print(f"   â„¹ï¸  ä¿¡æ¯: {quality_result['total_issues']['info']}")
        
        code_quality_score = quality_result['quality_score']['score']
        print(f"ğŸ“ ä»£ç è´¨é‡è¯„åˆ†: {code_quality_score}/10 ({quality_result['quality_score']['rating']})")
        
        assessment_results['code_quality'] = {
            'score': code_quality_score,
            'rating': quality_result['quality_score']['rating'],
            'details': quality_result
        }
        
    except Exception as e:
        print(f"âŒ ä»£ç è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
        code_quality_score = 0
        assessment_results['code_quality'] = {'score': 0, 'error': str(e)}
    
    # 2. ä»£ç è°ƒè¯•èƒ½åŠ›è¯„ä¼° (10åˆ†)
    print(f"\nğŸ› 2. ä»£ç è°ƒè¯•èƒ½åŠ›è¯„ä¼°")
    print("-" * 40)
    
    debug_score = assess_debugging_capabilities(project_files)
    assessment_results['code_debugging'] = debug_score
    
    print(f"ğŸ› ä»£ç è°ƒè¯•è¯„åˆ†: {debug_score['score']}/10 ({debug_score['rating']})")
    
    # 3. ç»¼åˆè¯„åˆ†è®¡ç®—
    overall_score = (code_quality_score + debug_score['score']) / 2
    assessment_results['overall_score'] = overall_score
    
    print(f"\nğŸ¯ ç»¼åˆè¯„ä¼°ç»“æœ")
    print("=" * 40)
    print(f"ğŸ“ ä»£ç è´¨é‡: {code_quality_score}/10")
    print(f"ğŸ› ä»£ç è°ƒè¯•: {debug_score['score']}/10")
    print(f"ğŸ† ç»¼åˆå¾—åˆ†: {overall_score:.1f}/10")
    
    # è¯„çº§
    if overall_score >= 9:
        overall_rating = "ä¼˜ç§€ ğŸŒŸ"
    elif overall_score >= 8:
        overall_rating = "è‰¯å¥½ ğŸ‘"
    elif overall_score >= 6:
        overall_rating = "ä¸€èˆ¬ â­"
    else:
        overall_rating = "éœ€æ”¹è¿› âš ï¸"
    
    print(f"ğŸ“Š æ€»ä½“è¯„çº§: {overall_rating}")
    
    # 4. ç”Ÿæˆæ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®")
    print("-" * 40)
    generate_improvement_suggestions(assessment_results)
    
    # 5. ä¿å­˜è¯„ä¼°æŠ¥å‘Š
    save_assessment_report(assessment_results)
    
    return assessment_results


def assess_debugging_capabilities(project_files):
    """è¯„ä¼°ä»£ç è°ƒè¯•èƒ½åŠ›"""
    debug_features = {
        'error_handling': 0,      # é”™è¯¯å¤„ç†
        'logging': 0,             # æ—¥å¿—è®°å½•
        'type_hints': 0,          # ç±»å‹æç¤º
        'documentation': 0,       # æ–‡æ¡£å®Œæ•´æ€§
        'test_coverage': 0,       # æµ‹è¯•è¦†ç›–
        'security_practices': 0   # å®‰å…¨å®è·µ
    }
    
    total_files = 0
    
    for file_path in project_files:
        if not file_path.endswith('.py') or not os.path.exists(file_path):
            continue
            
        total_files += 1
        
        try:
            with open(file_path, 'r', encoding='utf-8') as file_handle:
                content = file_handle.read()
            
            # æ£€æŸ¥é”™è¯¯å¤„ç†
            if 'try:' in content and 'except' in content:
                debug_features['error_handling'] += 1
            
            # æ£€æŸ¥ç±»å‹æç¤º
            if 'from typing import' in content or 'import typing' in content:
                debug_features['type_hints'] += 1
            
            # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
            if '"""' in content or "'''" in content:
                debug_features['documentation'] += 1
            
            # æ£€æŸ¥æ—¥å¿—è®°å½•
            if 'logging' in content or 'print(' in content:
                debug_features['logging'] += 1
            
            # æ£€æŸ¥å®‰å…¨å®è·µï¼ˆç®€åŒ–ç‰ˆï¼‰
            if 'validate' in content.lower() or 'sanitize' in content.lower():
                debug_features['security_practices'] += 1
                
        except Exception as e:
            print(f"âš ï¸  æ— æ³•åˆ†ææ–‡ä»¶ {file_path}: {e}")
    
    # æ£€æŸ¥æµ‹è¯•æ–‡ä»¶
    test_files = ['test_suite.py', 'test_*.py', '*_test.py']
    for pattern in test_files:
        if os.path.exists(pattern.replace('*', '')):
            debug_features['test_coverage'] = 1
            break
    
    # è®¡ç®—è°ƒè¯•èƒ½åŠ›è¯„åˆ†
    total_possible = total_files * 5 + 1  # 5ä¸ªç‰¹æ€§ + æµ‹è¯•è¦†ç›–
    total_achieved = sum(debug_features.values())
    
    debug_score = min(10, (total_achieved / total_possible) * 10) if total_possible > 0 else 0
    
    # è¯„çº§
    if debug_score >= 8:
        rating = "ä¼˜ç§€"
    elif debug_score >= 6:
        rating = "è‰¯å¥½"
    elif debug_score >= 4:
        rating = "ä¸€èˆ¬"
    else:
        rating = "éœ€æ”¹è¿›"
    
    print(f"   âœ… é”™è¯¯å¤„ç†: {debug_features['error_handling']}/{total_files} æ–‡ä»¶")
    print(f"   ğŸ·ï¸  ç±»å‹æç¤º: {debug_features['type_hints']}/{total_files} æ–‡ä»¶")
    print(f"   ğŸ“– æ–‡æ¡£å­—ç¬¦ä¸²: {debug_features['documentation']}/{total_files} æ–‡ä»¶")
    print(f"   ğŸ“ æ—¥å¿—è®°å½•: {debug_features['logging']}/{total_files} æ–‡ä»¶")
    print(f"   ğŸ”’ å®‰å…¨å®è·µ: {debug_features['security_practices']}/{total_files} æ–‡ä»¶")
    print(f"   ğŸ§ª æµ‹è¯•è¦†ç›–: {'âœ…' if debug_features['test_coverage'] else 'âŒ'}")
    
    return {
        'score': round(debug_score, 1),
        'rating': rating,
        'features': debug_features,
        'details': {
            'total_files': total_files,
            'coverage_percentage': round((total_achieved / total_possible) * 100, 1) if total_possible > 0 else 0
        }
    }


def generate_improvement_suggestions(results):
    """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    suggestions = []
    
    # åŸºäºä»£ç è´¨é‡ç»“æœçš„å»ºè®®
    if 'code_quality' in results and 'details' in results['code_quality']:
        quality_details = results['code_quality']['details']
        
        if quality_details['total_issues']['error'] > 0:
            suggestions.append("ğŸ”§ ä¿®å¤ä»£ç é”™è¯¯ï¼Œæé«˜ä»£ç ç¨³å®šæ€§")
        
        if quality_details['total_issues']['warning'] > 5:
            suggestions.append("âš ï¸  å‡å°‘ä»£ç è­¦å‘Šï¼Œæ”¹å–„ä»£ç è´¨é‡")
        
        if quality_details['total_metrics']['docstring_coverage'] < 70:
            suggestions.append("ğŸ“– å¢åŠ æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œæé«˜ä»£ç å¯è¯»æ€§")
    
    # åŸºäºè°ƒè¯•èƒ½åŠ›ç»“æœçš„å»ºè®®
    if 'code_debugging' in results:
        debug_details = results['code_debugging']
        
        if debug_details['features']['error_handling'] < debug_details['details']['total_files']:
            suggestions.append("ğŸ›¡ï¸  æ·»åŠ æ›´å¤šé”™è¯¯å¤„ç†æœºåˆ¶")
        
        if debug_details['features']['type_hints'] < debug_details['details']['total_files']:
            suggestions.append("ğŸ·ï¸  å¢åŠ ç±»å‹æç¤ºï¼Œæé«˜ä»£ç å®‰å…¨æ€§")
        
        if not debug_details['features']['test_coverage']:
            suggestions.append("ğŸ§ª æ·»åŠ å•å…ƒæµ‹è¯•ï¼Œæé«˜ä»£ç å¯é æ€§")
        
        if debug_details['features']['security_practices'] == 0:
            suggestions.append("ğŸ”’ åŠ å¼ºå®‰å…¨éªŒè¯å’Œæ•°æ®æ ¡éªŒ")
    
    # é€šç”¨å»ºè®®
    if results['overall_score'] < 8:
        suggestions.append("ğŸ“š å‚è€ƒæœ€ä½³å®è·µï¼ŒæŒç»­æ”¹è¿›ä»£ç è´¨é‡")
        suggestions.append("ğŸ” å»ºç«‹ä»£ç å®¡æŸ¥æµç¨‹")
    
    if not suggestions:
        suggestions.append("ğŸ‰ ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ!")
    
    for i, suggestion in enumerate(suggestions, 1):
        print(f"{i}. {suggestion}")


def save_assessment_report(results):
    """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
    report_file = f"quality_assessment_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    try:
        with open(report_file, 'w', encoding='utf-8') as report_file_handle:
            json.dump(results, report_file_handle, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print(f"âš ï¸  ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


def check_project_structure():
    """æ£€æŸ¥é¡¹ç›®ç»“æ„å®Œæ•´æ€§"""
    print("\nğŸ—ï¸  é¡¹ç›®ç»“æ„æ£€æŸ¥")
    print("-" * 40)
    
    required_files = {
        'recommendation_engine.py': 'æ¨èå¼•æ“æ ¸å¿ƒ',
        'data_sources.py': 'æ•°æ®æºç®¡ç†',
        'api_server.py': 'APIæœåŠ¡å™¨',
        'gui.py': 'å›¾å½¢ç•Œé¢',
        'config.json': 'é…ç½®æ–‡ä»¶',
        'requirements.txt': 'ä¾èµ–åˆ—è¡¨',
        'README.md': 'é¡¹ç›®æ–‡æ¡£'
    }
    
    structure_score = 0
    total_files = len(required_files)
    
    for file_path, description in required_files.items():
        if os.path.exists(file_path):
            print(f"âœ… {file_path} - {description}")
            structure_score += 1
        else:
            print(f"âŒ {file_path} - {description} (ç¼ºå¤±)")
    
    print(f"\nğŸ“Š ç»“æ„å®Œæ•´æ€§: {structure_score}/{total_files} ({structure_score/total_files*100:.1f}%)")
    return structure_score / total_files


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥é¡¹ç›®ç»“æ„
        structure_score = check_project_structure()
        
        # è¿è¡Œç»¼åˆè¯„ä¼°
        results = run_comprehensive_assessment()
        
        print(f"\nğŸ¯ æœ€ç»ˆè¯„ä¼°æ€»ç»“")
        print("=" * 60)
        print(f"ğŸ—ï¸  é¡¹ç›®ç»“æ„: {structure_score*10:.1f}/10")
        print(f"ğŸ“ ä»£ç è´¨é‡: {results['code_quality'].get('score', 0)}/10")
        print(f"ğŸ› ä»£ç è°ƒè¯•: {results['code_debugging']['score']}/10")
        print(f"ğŸ† ç»¼åˆè¯„åˆ†: {results['overall_score']:.1f}/10")
        
        # ç¬¦åˆè¯„ä¼°æ ‡å‡†åˆ¤æ–­
        code_quality_level = ""
        if results['code_quality'].get('score', 0) >= 8:
            code_quality_level = "ä»£ç è§„èŒƒï¼Œæ˜“è¯»ã€æ˜“ç»´æŠ¤ã€å¯æ‰©å±• (8-10åˆ†)"
        elif results['code_quality'].get('score', 0) >= 4:
            code_quality_level = "ä»£ç è´¨é‡ä¸€èˆ¬ï¼Œéƒ¨åˆ†æ»¡è¶³ç®¡ç†è¦æ±‚ (4-7åˆ†)"
        else:
            code_quality_level = "ä»£ç è´¨é‡å·®ï¼Œä¸ç¬¦åˆç®¡ç†è¦æ±‚ (0-3åˆ†)"
        
        debug_level = ""
        if results['code_debugging']['score'] >= 8:
            debug_level = "è°ƒè¯•ç®¡ç†å®Œå–„ï¼Œæ— ä¸¥é‡æ¼æ´ (8-10åˆ†)"
        elif results['code_debugging']['score'] >= 4:
            debug_level = "è°ƒè¯•ç®¡ç†ä¸€èˆ¬ï¼Œå­˜åœ¨éƒ¨åˆ†å¯æ¥å—çš„æ¼æ´ (4-7åˆ†)"
        else:
            debug_level = "è°ƒè¯•ç®¡ç†å·®ï¼Œå­˜åœ¨ä¸¥é‡æ¼æ´ (0-3åˆ†)"
        
        print(f"\nğŸ“‹ è¯„ä¼°æ ‡å‡†å¯¹ç…§:")
        print(f"ğŸ“ ä»£ç è´¨é‡: {code_quality_level}")
        print(f"ğŸ› ä»£ç è°ƒè¯•: {debug_level}")
        
        return results
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {e}")
        return None


if __name__ == '__main__':
    main() 